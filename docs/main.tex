%%
%% This is file `sample-sigplan.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigplan')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigplan.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigplan,screen]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmcopyright}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[]{}{}{}
% \acmBooktitle{}
% \acmPrice{}
% \acmISBN{}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.



\usepackage{biblatex}
\usepackage{graphicx}
\usepackage{csquotes}
\usepackage{bbm}
\addbibresource{acmart.bib}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Benchmarking the Direct Method for Off-Policy Bandit Evaluation}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Youngjin Park}
\email{yjp228@nyu.edu}
\affiliation{%
  \institution{New York University}
}

\author{Soojin Kim}
\email{sk5291@nyu.edu}
\affiliation{%
  \institution{New York University}
}

\author{Eric He}
\email{eh1885@nyu.edu}
\affiliation{%
  \institution{New York University}
}

\author{Tejomay Gadgil}
\email{tg1906@nyu.edu}
\affiliation{%
  \institution{New York University}
}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Gadgil, He, Kim, Park}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  We benchmark DM using a variety of models on commonly used bandit datasets and find that CONCLUSION HERE. Our code can be found at \url{https://github.com/EricHe98/direct\_method}.
\end{abstract}

% %%
% %% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% %% Please copy and paste the code instead of the example below.
% %%
% \begin{CCSXML}
% <ccs2012>
% <concept>
% <concept_id>10010147.10010257</concept_id>
% <concept_desc>Computing methodologies~Machine learning</concept_desc>
% <concept_significance>500</concept_significance>
% </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Computing methodologies~Machine learning}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
% \keywords{Bandits, Contextual Bandits, Off-policy evaluation, Direct Method}

\maketitle

\section{Introduction}
We are concerned with the problem of off-policy evaluation in the contextual bandit setting. The contextual bandit setting is as follows: given a \textit{context} $x \in \mathcal{X}$ drawn from a distribution $\Omega(x)$, one of finitely many possible \textit{actions} $a \in \mathcal{A}$ is selected according to a policy $\pi(a|x)$, and the action results in a real-valued \textit{reward} $r \in \mathbb{R}$ drawn from a distribution $\delta(r|a,x)$. For any given policy $\pi$, we can estimate the \textit{value} $V(\pi)$ of the policy by its expected reward\footnote{Usually, we regard $\Omega(x)$ as implicit and drop it from our notation: 

$$\mathbb{E}_{\pi}[\delta(r|a,x)] = \int_{a \in \mathcal{A}} \int_{r \in \mathbb{R}} \delta(r|a,x) \pi(a|x) \, dr \, da$$}:

\begin{align}
    V(\pi) &= \mathbb{E}_{\pi, \Omega(x)}[\delta(r|a,x)] \\ 
    &= \int_{x \in \mathcal{X}} \int_{a \in \mathcal{A}} \int_{r \in \mathbb{R}} \delta(r|a,x) \pi(a|x) \Omega(x) \, dr \, da \, dx
\end{align} 

The goal of off-policy evaluation is to estimate the value of a \textit{target} policy $\pi_1$ given a set of $n$ tuples $(x_0, a_0, r_0)$, $(x_1, a_1, r_1)$, ..., $(x_n, a_n, r_n)$ drawn from a \textit{logging} policy $\pi_0$. The primary challenge here is that there is a mismatch between the actions taken by the logging policy in the data and the actions that would've been taken by the target policy given the same context.

\subsection{An example of off-policy evaluation in contextual bandits}
An illustrative example of the contextual bandit setting is a doctor recommending treatments for patients with heart problems. The characteristics of the patient forms the context: for example, their age, height, weight, sex, reported level of pain and other relevant factors. The doctor's recommendation is the action. For example, the doctor could pick one of three recommendations: exercise, drugs, or surgery. The outcome of the patient is the reward. In this example, we could set it to $1$ if the patient was "cured" and $0$ otherwise. The doctor's criteria for recommendation forms the policy; for example, a policy could be to recommend exercise 75\% of the time to anyone below the age of 60, and drugs otherwise; for patients of age 60 and above, recommend drugs 60\% of the time and surgery otherwise.

We would like to evaluate a new doctor, but without going through the laborious process of giving recommendations to hundreds of patients and waiting to discover the outcome. One way this can be done is by grading the new doctor's treatment policy ($\pi_1)$ according to the historical outcomes of patients treated by another doctor ($\pi_0$). If the two doctors recommend the same treatment for a patient, then we know the historical outcome of that patient is an exact assessment of $\pi_1$ for that patient. The issue is that if the two doctors do not recommend the same treatment for a patient, it's not possible to determine exactly what the outcome of the new doctor's policy would've been, since the data only records the outcome of the treatment recommended by $\pi_0$.

\subsection{Approaches to off-policy evaluation}
We present the two basic approaches to off-policy evaluation: \textit{importance weighting} and \textit{the direct method}. These two approaches are usually combined in a method called \textit{doubly robust estimation}.

We note a key assumption of both methods: the logging policy must have support over all actions taken by the target policy. Mathematically, this means $\pi_0(a|x) > 0$ whenever $\pi_1(a|x) > 0$. Intuitively, a target policy's action can only be evaluated if there is some probability of seeing it under the logging policy; otherwise the reward would be entirely unknown because the logging policy has never explored it. In the doctor example, if the new doctor recommends heart surgery to a newborn infant when the original doctor never has, there is no way to determine the outcome of that treatment using the original doctor's historical data.

\subsubsection{Importance weighting}
Given our dataset $(x_0, a_0, r_0)$, $(x_1, a_1, r_1)$, ..., $(x_n, a_n, r_n)$ with the actions drawn from a logging policy $\pi_0$, the sample mean $\sum_{i=1}^n r_i$ is an unbiased estimator of the logging policy's value $V(\pi_0)$. Thus, our empirical estimate of $V(\pi_0)$ can be written as 

\begin{equation}
    \hat{V}(\pi_0) = \dfrac{1}{n} \sum_{i=1}^n r_i
\end{equation}

However, what we are interested in is an estimate of $V(\pi_1)$. Importance weighting corrects the action sampling bias from the logging policy using a change of distribution to the target policy. Each data point is weighted by the relative likelihood of being seen in the target policy vs. the logging policy. Mathematically, this can be written as 

\begin{align}
    &V(\pi_1) = \mathbb{E}_{\pi_1}[\delta(r|a,x)] \\ 
    &= \int_{a \in \mathcal{A}} \int_{r \in \mathbb{R}} \delta(r|a,x) \pi_1(a|x) \, dr \, da  \\ 
    &= \int_{a \in \mathcal{A}} \int_{r \in \mathbb{R}} \delta(r|a,x) \pi_1(a|x) \dfrac{\pi_0(a|x)}{\pi_0(a|x)} \, dr \, da \\
    &= \mathbb{E}_{\pi_0}[\delta(r|a,x) \dfrac{\pi_1(a|x)}{\pi_0(a|x)}]
\end{align}

and the empirical estimate of $V(\pi_1)$ would be 

\begin{equation}
    \hat{V}_{IW}(\pi_1) = \dfrac{1}{n} \sum_{i=1}^n r_i \dfrac{\pi_1(a|x)}{\pi_0(a|x)}
\end{equation}

Importance weighting is guaranteed to give an unbiased estimate of $V(\pi_1)$.

The importance weights $\frac{\pi_1(a|x)}{\pi_0(a|x)}$ can take on any positive value; large importance weights can introduce significant variance into the value estimates. Reducing the variance around the estimates given by the standard importance weighting method is an active area of research, but not the focus of this paper.

\subsubsection{Direct method}
Another well-known way to estimate rewards in a contextual bandit setting is via the direct method ("DM") of fitting a model to the rewards given the context and action. This simply casts rewards prediction problem as a standard supervised regression problem which can be solved with any off-the-shelf model.

For example, suppose we implemented the direct method by training $|\mathcal{A}|$ separate linear regressions $f_{a_1}(x), ..., f_{a_{|\mathcal{A}|}(x)}$ with weights $\theta_{a_1}, ..., \theta_{a_{|\mathcal{A}|}(x)}$ for each action with the square loss function. Then the training objective for the $k$th linear regression could be written as 

\begin{align}
\text{arg}\,\text{min}_{\theta_k \in \Theta} \dfrac{1}{n} \sum_{i=1}^n (r_i - \theta^T_k x_i)^2\mathbbm{1}[a_i = k]
\end{align}

where $\mathbbm{1}[a_i = k]$ is the indicator function taking value $1$ if the $i$th action is $k$ and $0$ otherwise.

One flaw with the training objective written as is is that the model is trained on the logging policy's action distribution, and will try to minimize rewards error according to that action distribution. To get a model to minimize rewards error according to the target policy's action distribution, we can incorporate importance weighting into the loss function:

\begin{align}
\text{arg}\,\text{min}_{\theta_k \in \Theta} \dfrac{1}{n} \sum_{i=1}^n (r_i - \theta^T_k x_i)^2\mathbbm{1}[a_i = k] \dfrac{\pi_1(a_k|x_i)}{\pi_0(a_k|x_i)}
\end{align}

Given a fitted rewards estimator $\hat{f}_\theta(x_i, a_i)$, we can obtain our desired estimate $\hat{V}(\pi_1)$ by summing the predicted rewards for each action, weighted by $\pi_1$, over the values of $x$ in our dataset. Mathematically, this would be written as 

\begin{equation}
    \hat{V}_{DM}(\pi_1) = \dfrac{1}{n} \sum_{i=1}^n \sum_{k=1}^{|\mathcal{A}|} \hat{f}_\theta(x_i, a_k) \pi_1(a_k|x_i)
\end{equation}

If $\hat{f}_\theta$ is an unbiased estimator of $\delta(r|a,x) \forall a, x$, then $\hat{V_{DM}(\pi_1)}$ will also be unbiased. 

\subsubsection{Doubly robust estimation}
Doubly robust estimation combines importance weighting and the direct method. There are many doubly robust estimators; an example of one is 

\begin{align}
    &\hat{V}_{DR}(\pi_1) = \dfrac{1}{n} \sum_{i=1}^n\bigg(\hat{V}_{DM}^{x_i}(\pi_1) +  \frac{\pi_1(a_i|x_i)}{\pi_0(a_i|x_i)}(r_i - \hat{f}_\theta(x_i, a_i))\bigg), \\
    & \hat{V}_{DM}^{x_i}(\pi_1) = \sum_{k=1}^{|\mathcal{A}|} \hat{f}_\theta(x_i, a_k) \pi_1(a_k|x_i)
\end{align}

The doubly robust estimator has some nice properties. The estimate will be strictly better than IW or DM individually, and has lower variance than IW when the DM estimate is correlated with the true reward. Most interest around DM methods can be attributed to their utility in doubly robust estimators.

\subsection{Utility of the direct method}
DM is historically presented as an inaccurate method for off-policy evaluation, generally only useful in comparison to other approaches or as part of a doubly robust estimator.

From \cite{dudik}, the foundational paper on doubly robust estimation:

\begin{displayquote}
... the IPS estimator is in practice less susceptible to problems with bias compared with the direct method. However, IPS typically has a much larger variance...
\end{displayquote}

From \cite{poem}, the paper presenting the POEM estimator commonly used today:

\begin{displayquote}
In principle, since the logs give us an incomplete view of the feedback for different predictions, one could first use regression to estimate a feedback oracle for unseen predictions, and then use any supervised learning algorithm using this feedback oracle. Such a two-stage approach is known to not generalize well.
\end{displayquote}

However, such papers tend to use simple and high-bias learning methods such as linear regression when implementing DM. In principle, complex nonlinear models such as neural nets or tree ensembles can be extremely effective DM estimators if they are able to produce accurate estimates of $\delta(r|a,x)$. Indeed, in the past decade, such models have seen much greater adoption and performance improvements on supervised learning problems. Recent research such as \cite{overparameterize1} has suggested that more complex models can give better performance on contextual bandits problems as well.

This paper contributes an empirical benchmark of the utility of DM on a number of datasets, using not just standard linear regression but also more complex nonlinear models such as gradient boosting and neural networks. 

\section{Related Work}
The contextual bandits setting has rich ties to reinforcement learning, causal inference, missing value imputation, and other problems which have some concept of distribution mismatch. \cite{dudik} established bounds on the bias and variance of doubly robust value estimators in the contextual bandits setting and provided the framework for translating full feedback datasets into bandit datasets which we use for our experiments; \cite{kang_schafer} and \cite{jiang} do the same in the causal inference and reinforcement learning settings, respectively.

Estimators have been designed specifically for rewards prediction in partial feedback settings. The offset tree algorithm introduced in \cite{offset_tree} is used by \cite{dudik} for constructing doubly robust estimators; it trains a series of binary classifiers to distinguish whether to pick one action over another. The Q-learning algorithm introduced in \cite{Watkins92q-learning} is a dynamic programming algorithm for producing estimates of the expected future reward of a given action and state in a reinforcement learning setting. For this paper, however, we are interested in using standard off-the-shelf regression and classification models.

Methods which use complex models to perform rewards estimation have seen great success. \cite{joachims2018deep} introduced BanditNet, a method to train deep neural networks on contextual bandit feedback using a counterfactual risk minimization objective. \cite{deepq} presented the deep Q-learning\footnote{We call the rewards function $\delta(a|x,y)$, but in reinforcement learning, the function determining the expected future rewards for an action is called the $Q$ function.} method in a reinforcement learning setting, in which a deep convolutional neural network is used to estimate the reward function in playing a game of Atari. \cite{overparameterize1} presents a theoretical decomposition of excess risk into approximation error, estimation error, and "bandit error", in which IW methods are susceptible to bandit error but DM is not. 

\section{Datasets and Methodology}
Natural bandit datasets are difficult to come by; this paper benchmarks on only two true bandit datasets, "Zozo" and "Hotels". To supplement this, we use synthetic regression datasets ("SeaVans" and "News") and publicly available classification datasets ("Yeast", "Statlog (Shuttle)", "DryBean", "Letter"). A high-level description of each dataset is given in Table~\ref{Dataset-characteristics}; we describe each dataset class in more detail below.

\begin{table*}
  \caption{Dataset characteristics}
  \label{Dataset-characteristics}
\begin{tabular}{lrrrr}
\toprule
Name & Type & Data Points  & Feature Count  \\ 
\midrule
Yeast & Classification (10) & 1484 &   \\
Statlog (Shuttle) &  Classification (9) & 58000 & \\
DryBean & Classification (7) & 13611 &  \\
Letter & Classification (26) & 20000  &  \\
Zozo & Bandit & 26000000 &  \\
News  & Regression & N &  \\
Hotels & Bandit & N & \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Evaluating on Classification Datasets}
\subsubsection{From full feedback to bandit feedback}
We mirror the procedure of \cite{dudik} in converting a classification dataset into a bandit dataset. Suppose the dataset has $K$ classes. Then each data point in the classification dataset can be written as $(x, l)$ where $x$ is the feature vector and $l \in {1,...,K}$ is the label corresponding to the correct class of $x$. 

This data point can be converted into a bandit data point by:

\begin{enumerate}
    \item keeping $x$ as is.
    \item mapping the action set $\mathcal{A}$ to the $K$ class labels ${1,...,K}$.
    \item selecting an action $a \in {1, ..., K}$ according to an as-yet undetermined policy.
    \item setting the reward $r$ to be $1$ if the selected action matches the correct class and $0$ otherwise: $\delta(r_i|a_i,x_i) = \mathbbm{1}[a_i = l_i]$.
\end{enumerate}

This methodology gives us full freedom to choose the logging and target policies.

\subsubsection{Setting the target policy}
The target policy can be an arbitrary function $\pi_1(a|x)$. We take a similar approach to \cite{dudik} and \cite{pmlr-v97-vlassis19a}. First, we split the dataset into training and testing subsets with $70\%$ and $30\%$ of the data, respectively. Then, we train a classification model $\nu(x)$ on the train set with full feedback labels, and use that model's predicted probabilities as the target policy. The model's classification accuracy on the test set is analogous to the target policy's value.

The choice of setting our target policy to the predictions of a classification model trained on the full feedback replicates the intuition that the target policy should ideally be an improvement over the logging policy. We use logistic regression and gradient boosting trees as target policy.

\subsubsection{Setting the logging policy}
For one set of choices of logging policy, we again follow \cite{dudik}, which is as follows: with probability $\epsilon$ we select the "correct" action with reward $1$, and with probability $1 - \epsilon$ we sample uniformly across the incorrect actions. We vary $\epsilon$ to take different values between $0.1$ and $0.7$. This choice is intended to artificially replicate the notion that a logging policy has an imperfect, but better-than-random idea of the correct action to take. We term these logging policies "$\epsilon$-correct logging policies".

Another set of logging policies randomly drops a proportion $\rho$ of the features in the training set before training on the full feedback. Then, the predicted probabilities of that model is used as the target policy. This choice is intended to artificially simulate the effect of an earlier logging policy not using as much of the data as a new target policy. We term these logging policies "$\rho$-featured logging policies".

Given these logging policies, we transform the training set into a bandit dataset, and use the partial feedback of the bandit dataset to build value estimators.

\subsubsection{Rewards estimation}

















\section{Evaluation}


\section{Models}


\section{Results}


\section{Discussion}

\section{Conclusion}

\printbibliography
\end{document}
\endinput
%%
%% End of file `sample-sigplan.tex'.
